{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dd10195-aade-4f72-8037-dcd5f1ccdd3c",
   "metadata": {},
   "source": [
    "## Imports\n",
    "This installs all required tools and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1b247fe-629c-4512-a667-971c2cbd085e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.10.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy\n",
    "import os\n",
    "import time\n",
    "import pygame\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from maze_game import Game\n",
    "from model import Linear_QNet, QTrainer\n",
    "from IPython import get_ipython\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed88eb95-7cd4-488b-8acc-d2b0d74eed91",
   "metadata": {},
   "source": [
    "## Maze Game Class\n",
    "This is responsible for storing and visualizing the agent's environment. It also calculates reward values for each move the agent makes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7810dabb-0ee9-4b18-91b1-47aa7a2b4da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceFileDir = os.path.dirname(os.path.abspath(\"\"))\n",
    "switch_mazes = False\n",
    "\n",
    "class Game():\n",
    "    SIZE = (500, 500)\n",
    "    TILE_SIZE = 45.5\n",
    "\n",
    "    def __init__(self):\n",
    "        pygame.init()\n",
    "        self.screen = pygame.display.set_mode(Game.SIZE)\n",
    "        self.textures = [\n",
    "            pygame.transform.scale(pygame.image.load(os.path.join(sourceFileDir, \"reinforcement_learning/resources/wall.jpg\")), (Game.TILE_SIZE + 1, Game.TILE_SIZE + 1)),\n",
    "            pygame.transform.scale(pygame.image.load(os.path.join(sourceFileDir, \"reinforcement_learning/resources/road.jpg\")), (Game.TILE_SIZE + 1, Game.TILE_SIZE + 1)),\n",
    "            pygame.transform.scale(pygame.image.load(os.path.join(sourceFileDir, \"reinforcement_learning/resources/path.png\")), (Game.TILE_SIZE + 1, Game.TILE_SIZE + 1)),\n",
    "            pygame.transform.scale(pygame.image.load(os.path.join(sourceFileDir, \"reinforcement_learning/resources/player.png\")), (Game.TILE_SIZE + 1, Game.TILE_SIZE + 1))\n",
    "        ]\n",
    "\n",
    "        self.level = 1\n",
    "        self.load_level(self.level)\n",
    "        self.reset()\n",
    "\n",
    "    # Loads tilemap from .maze type file and sets up the environment\n",
    "    def load_level(self, level):\n",
    "        self.tilemap = numpy.loadtxt(os.path.join(sourceFileDir, \"reinforcement_learning/dataset/matrices/\" + str(level) + \".maze\"), delimiter=',', dtype=numpy.int8)\n",
    "        self.map_width = len(self.tilemap[0])\n",
    "        self.map_height = len(self.tilemap)\n",
    "        self.min_reward = -0.5 * self.map_width * self.map_height\n",
    "        self.num_games = 0\n",
    "\n",
    "    # Resets the environment on game over\n",
    "    def reset(self):\n",
    "        self.path = [[1, 1]]\n",
    "        self.pathed_tilemap = self.tilemap.copy().astype(float)\n",
    "        self.pathed_tilemap[1][1] = 0.3 # Starting position\n",
    "        self.moves = 0\n",
    "        self.total_reward = 0\n",
    "\n",
    "    # Performs visual updates to Pygame display\n",
    "    def run(self):\n",
    "        self.draw_map()\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "            if event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_UP:\n",
    "                    self.play_step(1, 0, 0, 0)\n",
    "                elif event.key == pygame.K_RIGHT:\n",
    "                    self.play_step(0, 1, 0, 0)\n",
    "                elif event.key == pygame.K_DOWN:\n",
    "                    self.play_step(0, 0, 1, 0)\n",
    "                elif event.key == pygame.K_LEFT:\n",
    "                    self.play_step(0, 0, 0, 1)\n",
    "        self.draw_path()\n",
    "        pygame.display.update()\n",
    "\n",
    "    # Draws the map\n",
    "    def draw_map(self):\n",
    "        for row in range(self.map_height):\n",
    "            for column in range(self.map_width):\n",
    "                self.screen.blit(self.textures[self.tilemap[row][column]], (column*Game.TILE_SIZE, row*Game.TILE_SIZE))\n",
    "\n",
    "    # Draws the agent's path\n",
    "    def draw_path(self):\n",
    "        for i, coord in enumerate(self.path):\n",
    "            if i < len(self.path) - 1:\n",
    "                self.screen.blit(self.textures[2], (coord[0]*Game.TILE_SIZE, coord[1]*Game.TILE_SIZE))\n",
    "            else:\n",
    "                self.screen.blit(self.textures[3], (coord[0]*Game.TILE_SIZE, coord[1]*Game.TILE_SIZE))\n",
    "\n",
    "    # Performs the action requested, calculates reward value, returns outcome\n",
    "    def play_step(self, up, right, down, left):\n",
    "        self.moves += 1\n",
    "\n",
    "        # Make step\n",
    "        next_coord = self.path[len(self.path) - 1].copy()\n",
    "        if up:\n",
    "            next_coord[1] -= 1\n",
    "        elif right:\n",
    "            next_coord[0] += 1\n",
    "        elif down:\n",
    "            next_coord[1] += 1\n",
    "        elif left:\n",
    "            next_coord[0] -= 1\n",
    "\n",
    "        # Calculate reward\n",
    "        reward = -0.04\n",
    "        game_over = False\n",
    "        ignore_move = False\n",
    "        if next_coord == [9, 9]: # Handle sucessful finish\n",
    "            reward = 1.0\n",
    "            game_over = True\n",
    "        elif self.tilemap[next_coord[1]][next_coord[0]] == 0: # Handle collision\n",
    "            reward = -0.75\n",
    "            ignore_move = True\n",
    "        elif next_coord in self.path: # Handle repeat path\n",
    "            reward = -0.25\n",
    "\n",
    "        if self.total_reward < self.min_reward: # Handle run out of time\n",
    "            game_over = True\n",
    "\n",
    "        if not ignore_move:\n",
    "            last_coord = self.path[len(self.path) - 1]\n",
    "            self.path.append(next_coord)\n",
    "\n",
    "            self.pathed_tilemap[last_coord[1]][last_coord[0]] = 0.7\n",
    "            self.pathed_tilemap[next_coord[1]][next_coord[0]] = 0.3\n",
    "\n",
    "        self.run()\n",
    "\n",
    "        # Handles resets if nessesary, after rendering the finished display\n",
    "        moves = self.moves\n",
    "        if game_over:\n",
    "            self.num_games += 1\n",
    "            self.reset()\n",
    "\n",
    "        if switch_mazes and reward == 1.0 and self.num_games % 20 == 0:\n",
    "            self.level += 1\n",
    "            self.load_level(self.level)\n",
    "\n",
    "        self.total_reward += reward\n",
    "        return self.pathed_tilemap.copy(), reward, game_over, moves, self.level\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00de921b-af14-4470-bed2-667f5ab44dc0",
   "metadata": {},
   "source": [
    "## Model Classes\n",
    "This defines the Convolutional Q Net used for prediction, as well as the trainer to perform the Bellman equation and update the weights and biases of the net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "228dcba3-a1d4-4059-9bb5-7d6856d244c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.set_printoptions(linewidth=400)\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(1, 4, 3, padding = 1)\n",
    "        self.conv2 = torch.nn.Conv2d(4, 8, 3, stride = 1, padding = 1)\n",
    "        self.pool = torch.nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(200, 512)\n",
    "        self.linear2 = torch.nn.Linear(512, 128)\n",
    "        self.linear3 = torch.nn.Linear(128, 4)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Convolution Block\n",
    "        output = self.conv1(input)\n",
    "\n",
    "        output = torch.nn.functional.leaky_relu(output)\n",
    "        output = self.conv2(output)\n",
    "\n",
    "        output = torch.nn.functional.leaky_relu(output)\n",
    "        output = self.pool(output)\n",
    "\n",
    "        # Flatten\n",
    "        output = torch.flatten(output, 1)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        output = self.linear1(output)\n",
    "\n",
    "        output = torch.nn.functional.leaky_relu(output) \n",
    "        output = self.linear2(output)\n",
    "\n",
    "        output = torch.nn.functional.leaky_relu(output) \n",
    "        output = self.linear3(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def save(self, file_name='model.pth'):\n",
    "        model_folder_path = './model'\n",
    "        if not os.path.exists(model_folder_path):\n",
    "            os.makedirs(model_folder_path)\n",
    "\n",
    "        file_name = os.path.join(model_folder_path, file_name)\n",
    "        torch.save(self.state_dict(), file_name)\n",
    "\n",
    "class QTrainer():\n",
    "    def __init__(self, model, lr, gamma):\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.training_batch_size = 25\n",
    "        self.model = model\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=self.lr)\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "\n",
    "    # Trains the model with the given values\n",
    "    def train_step(self, state, action, reward, next_state, done):\n",
    "        # Create Batches\n",
    "        state_batches = numpy.array_split(state, self.training_batch_size)\n",
    "        action_batches = numpy.array_split(action, self.training_batch_size)\n",
    "        reward_batches = numpy.array_split(reward, self.training_batch_size)\n",
    "        next_state_batches = numpy.array_split(next_state, self.training_batch_size)\n",
    "        done_batches = numpy.array_split(done, self.training_batch_size)\n",
    "\n",
    "        for i in range(len(state_batches)):\n",
    "            state = torch.tensor(state_batches[i], dtype=torch.float)\n",
    "            state = torch.unsqueeze(state, 1)\n",
    "            action = torch.tensor(action_batches[i], dtype=torch.long)\n",
    "            reward = torch.tensor(reward_batches[i], dtype=torch.float)\n",
    "            next_state = torch.tensor(next_state_batches[i], dtype=torch.float)\n",
    "            next_state = torch.unsqueeze(next_state, 1)\n",
    "            done = done_batches[i]\n",
    "\n",
    "            # Predict Q values with current state\n",
    "            pred = self.model(state)\n",
    "            target = pred.clone()\n",
    "\n",
    "            # Perform Bellman Equation\n",
    "            for idx in range(len(done)):\n",
    "                Q_new = reward[idx]\n",
    "\n",
    "                if not done[idx]:\n",
    "                    next_pred = self.model(torch.unsqueeze(next_state[idx], 0))\n",
    "                    Q_new = reward[idx] + self.gamma * torch.max(next_pred)\n",
    "\n",
    "                target[idx][torch.argmax(action[idx]).item()] = Q_new\n",
    "\n",
    "            # Loss/Optimization Step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.criterion(target, pred)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d63ae2-06b3-4d16-b78a-cf18a5f056bc",
   "metadata": {},
   "source": [
    "## Agent Class\n",
    "This is the controller class that binds the environment and model together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aa2611b-a6ec-4082-8aa6-89d41e5d5bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MEMORY = 1500\n",
    "BATCH_SIZE = 100\n",
    "LR = 0.001\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.n_games = 0\n",
    "        self.epsilon = 0.1 # randomness\n",
    "        self.gamma = 0.95 # discout rate\n",
    "        self.memory = deque(maxlen=MAX_MEMORY) # double ended queue\n",
    "        self.model = QNet()\n",
    "        self.trainer = QTrainer(self.model, lr=LR, gamma=self.gamma)\n",
    "\n",
    "        plt.ion()\n",
    "\n",
    "    # Gets the current agent state\n",
    "    def get_state(self, game):\n",
    "        return numpy.array(game.pathed_tilemap, dtype=float)\n",
    "\n",
    "    # Add value to long term memory\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # Train long term memory\n",
    "    def train_model(self):\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            mini_sample = random.sample(self.memory, BATCH_SIZE) # list of tuples\n",
    "        else:\n",
    "            mini_sample = self.memory\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*mini_sample)\n",
    "        for i in range(8):\n",
    "            self.trainer.train_step(states, actions, rewards, next_states, dones)\n",
    "\n",
    "    # Predict based on state and return move\n",
    "    def get_action(self, state):\n",
    "        # random moves: tradeoff exploration / explotation\n",
    "        final_move = [0, 0, 0, 0]\n",
    "        if self.n_games < 1500 and random.uniform(0, 1) < self.epsilon:\n",
    "            move = random.randint(0, 3)\n",
    "            final_move[move] = 1\n",
    "        else:\n",
    "            state0 = torch.tensor(numpy.expand_dims(numpy.expand_dims(state, axis=0), axis=1), dtype=torch.float)\n",
    "            prediction = self.model(state0)\n",
    "            move = torch.argmax(prediction).item()\n",
    "            final_move[move] = 1\n",
    "\n",
    "        return final_move\n",
    "\n",
    "# The training function\n",
    "def train():\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=1)\n",
    "    ax1, ax2, ax3 = axes\n",
    "    plt.subplots_adjust(hspace=0.45, right=0.84)\n",
    "    plt.show(block=False)\n",
    "\n",
    "    plot_collisions = [0]\n",
    "    plot_backtracks = [0]\n",
    "    plot_total_moves = [0]\n",
    "    plot_wins = [0]\n",
    "    plot_n_games = [0]\n",
    "    plot_times = [0]\n",
    "\n",
    "    agent = Agent()\n",
    "    game = Game()\n",
    "\n",
    "    win_count = 0\n",
    "    total_time = 0\n",
    "\n",
    "    total_collisions = 0\n",
    "    total_backtracks = 0\n",
    "    last_time = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        # get old state\n",
    "        state_old = agent.get_state(game)\n",
    "\n",
    "        # get move\n",
    "        final_move = agent.get_action(state_old)\n",
    "\n",
    "        # perform move and get new state\n",
    "        state_new, reward, done, total_moves, score = game.play_step(final_move[0], final_move[1], final_move[2], final_move[3])\n",
    "        if reward == -0.75:\n",
    "            total_collisions += 1\n",
    "        elif reward == -0.25:\n",
    "            total_backtracks += 1\n",
    "\n",
    "        # remember\n",
    "        agent.remember(state_old, final_move, reward, state_new, done)\n",
    "\n",
    "        # train model\n",
    "        agent.train_model()\n",
    "\n",
    "        if done:\n",
    "            total_time = time.time() - start_time\n",
    "            agent.n_games += 1\n",
    "            agent.model.save()\n",
    "\n",
    "            win = False\n",
    "            if reward == 1.0:\n",
    "                win_count += 1\n",
    "\n",
    "            print('Game:', agent.n_games, '| Score:', score, '| Total Moves:', total_moves, '| Collisions:', total_collisions, '| Backtracks:', total_backtracks, '| Wins:', win_count, '| Delta Time:', round((total_time - last_time) / 60, 2), '| Total Time:', round(total_time / 60, 2))\n",
    "\n",
    "            plot_collisions.append(total_collisions)\n",
    "            plot_backtracks.append(total_backtracks)\n",
    "            plot_total_moves.append(total_moves)\n",
    "            plot_wins.append(win_count)\n",
    "            plot_n_games.append(agent.n_games)\n",
    "            plot_times.append(round((total_time - last_time) / 60, 2))\n",
    "\n",
    "            ax1.clear()\n",
    "            ax1.set_title('Algorithm Effectiveness')\n",
    "            ax1.set_xlabel('Game Number')\n",
    "            ax1.set_ylabel('Number of Actions')\n",
    "            ax1.plot(plot_n_games, plot_total_moves, label=\"Total Moves\")\n",
    "            ax1.plot(plot_n_games, plot_collisions, label=\"Collisions\")\n",
    "            ax1.plot(plot_n_games, plot_backtracks, label=\"Backtracks\")\n",
    "            ax1.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "            ax2.clear()\n",
    "            ax2.set_title('Win Rate')\n",
    "            ax2.set_xlabel('Game Number')\n",
    "            ax2.set_ylabel('Number of Winning Games')\n",
    "            ax2.plot(plot_n_games, plot_wins)\n",
    "\n",
    "            ax3.clear()\n",
    "            ax3.set_title('Elaspsed Time')\n",
    "            ax3.set_xlabel('Game Number')\n",
    "            ax3.set_ylabel('Time to Complete (minutes)')\n",
    "            ax3.plot(plot_n_games, plot_times)\n",
    "\n",
    "            total_collisions = 0\n",
    "            total_backtracks = 0\n",
    "            last_time = total_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1725cdb7-5046-4927-9198-c024882d7c02",
   "metadata": {},
   "source": [
    "## Launching Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cadcd990-487d-4f80-af5e-55df96c921f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game: 1 | Score: 1 | Total Moves: 122 | Collisions: 72 | Backtracks: 23 | Wins: 0 | Delta Time: 1.23 | Total Time: 1.23\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 79\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m final_move \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_action(state_old)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# perform move and get new state\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m state_new, reward, done, total_moves, score \u001b[38;5;241m=\u001b[39m \u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_move\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_move\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_move\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_move\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reward \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.75\u001b[39m:\n\u001b[1;32m     81\u001b[0m     total_collisions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[3], line 108\u001b[0m, in \u001b[0;36mGame.play_step\u001b[0;34m(self, up, right, down, left)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpathed_tilemap[last_coord[\u001b[38;5;241m1\u001b[39m]][last_coord[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.7\u001b[39m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpathed_tilemap[next_coord[\u001b[38;5;241m1\u001b[39m]][next_coord[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.3\u001b[39m\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Handles resets if nessesary, after rendering the finished display\u001b[39;00m\n\u001b[1;32m    111\u001b[0m moves \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoves\n",
      "Cell \u001b[0;32mIn[3], line 58\u001b[0m, in \u001b[0;36mGame.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28;01melif\u001b[39;00m event\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;241m==\u001b[39m pygame\u001b[38;5;241m.\u001b[39mK_LEFT:\n\u001b[1;32m     57\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplay_step(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mupdate()\n",
      "Cell \u001b[0;32mIn[3], line 69\u001b[0m, in \u001b[0;36mGame.draw_path\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, coord \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath):\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 69\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscreen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtextures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoord\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mGame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTILE_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoord\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mGame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTILE_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen\u001b[38;5;241m.\u001b[39mblit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtextures[\u001b[38;5;241m3\u001b[39m], (coord[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39mGame\u001b[38;5;241m.\u001b[39mTILE_SIZE, coord[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39mGame\u001b[38;5;241m.\u001b[39mTILE_SIZE))\n",
      "\u001b[0;31merror\u001b[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
